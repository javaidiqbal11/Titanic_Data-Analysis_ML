import numpy as np 
import pandas as pd 
import os
train_data = pd.read_csv("titanic_train.csv")
train_data.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.countplot(x='Survived', data=train_data)
plt.title('Survival Count')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.countplot(x='Pclass', data=train_data, order=train_data['Pclass'].value_counts().index)
plt.title('Distribution of Pclass')
plt.xlabel('Pclass')
plt.ylabel('Count')
plt.show()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.countplot(x='Sex', data=train_data, order=train_data['Sex'].value_counts().index)
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.countplot(x='SibSp', data=train_data, order=train_data['SibSp'].value_counts().index)
plt.title('Distribution of SibSp')
plt.xlabel('SibSp')
plt.ylabel('Count')
plt.show()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.histplot(train_data['Fare'], bins=20, kde=True)
plt.title('Distribution of Fare')
plt.xlabel('Fare')
plt.ylabel('Frequency')
plt.show()


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.countplot(x='Embarked', data=train_data, order=train_data['Embarked'].value_counts().index)
plt.title('Distribution of Embarked')
plt.xlabel('Embarked')
plt.ylabel('Count')
plt.show()


train_data.info()

train_data.isnull().sum()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

median_age = train_data['Age'].median()
train_data['Age'].fillna(median_age, inplace=True)

plt.figure(figsize=(8, 6))
sns.histplot(train_data['Age'], bins=20, kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()


train_data.head()

train_data.isnull().sum()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

top_10_frequent = train_data['Cabin'].value_counts().head(10)

print("Top 10 most frequent cabins:")
print(top_10_frequent)

most_common_cabin = train_data['Cabin'].mode()[0]
train_data['Cabin'].fillna(most_common_cabin, inplace=True)

plt.figure(figsize=(12, 6))
sns.countplot(x='Cabin', data=train_data, order=top_10_frequent.index)
plt.title('Top 10 Most Frequent Cabins')
plt.xlabel('Cabin')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


train_data.isnull().sum()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8, 6))
sns.countplot(x='Survived', hue='Sex', data=train_data)
plt.title('Survival Count by Sex')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.show()


test_data = pd.read_csv("titanic_test.csv")
test_data.head()

women = train_data.loc[train_data.Sex == 'female']["Survived"]
rate_women = sum(women)/len(women)

print("% of women who survived:", rate_women)

men = train_data.loc[train_data.Sex == 'male']["Survived"]
rate_men = sum(men)/len(men)

print("% of men who survived:", rate_men)

from sklearn.ensemble import RandomForestClassifier

y = train_data["Survived"]

features = ["Pclass", "Sex", "SibSp", "Parch"]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(X, y)
predictions = model.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})
output.to_csv('submission.csv', index=False)
print("Your submission was successfully saved!")

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Assuming 'train_data' and 'test_data' are your DataFrames containing the datasets

y = train_data["Survived"]
features = ["Pclass", "Sex", "SibSp", "Parch"]
X = pd.get_dummies(train_data[features])
X_test = pd.get_dummies(test_data[features])

model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(X, y)
predictions = model.predict(X_test)

# Create a DataFrame with PassengerId and corresponding predictions
output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})

# Print the DataFrame containing the predictions
print(output)


import matplotlib.pyplot as plt

# Assuming 'output' is your DataFrame containing the predictions

# Countplot to visualize the distribution of predicted survival values
plt.figure(figsize=(8, 6))
sns.countplot(x='Survived', data=output)
plt.title('Predicted Survival Distribution')
plt.xlabel('Survived')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['No', 'Yes'])
plt.tight_layout()
plt.show()


import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif

# Load the Titanic dataset
train_data = pd.read_csv('titanic_train.csv')

# Feature Engineering: Create a new feature 'FamilySize' by combining 'SibSp' and 'Parch'
train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']

# Encoding categorical variables using OneHotEncoder
categorical_features = ['Sex', 'Embarked']
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(train_data[categorical_features])

# Scaling numerical features using StandardScaler
numerical_features = ['Fare', 'Age']
scaler = StandardScaler()
scaled_features = scaler.fit_transform(train_data[numerical_features])

# Feature Selection: Select top features using ANOVA F-value
selector = SelectKBest(score_func=f_classif, k=3)
selected_features = selector.fit_transform(train_data[['Pclass', 'FamilySize', 'Fare']], train_data['Survived'])

# Prepare transformed DataFrame
transformed_data = pd.DataFrame(encoded_features.toarray(), columns=encoder.get_feature_names_out(categorical_features))
transformed_data[numerical_features] = scaled_features
transformed_data[['Pclass', 'FamilySize', 'Fare']] = selected_features

# Save the transformed data to a CSV file
transformed_data.to_csv('transformed_data.csv', index=False)

print("Transformed data saved to 'transformed_data.csv'.")


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('titanic_train.csv')

# Select features and target variable
X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]
y = data['Survived']

# Preprocessing: Handle missing values and encode categorical variables
encoder = OneHotEncoder(drop='first')
imputer = SimpleImputer(strategy='mean')
scaler = StandardScaler()

X_encoded = encoder.fit_transform(X[['Sex', 'Embarked']])
X_imputed = imputer.fit_transform(X[['Age']])
X_scaled = scaler.fit_transform(X[['Pclass', 'SibSp', 'Parch', 'Fare']])

# Create DataFrame for encoded features with proper column names
encoded_columns = encoder.get_feature_names_out(['Sex', 'Embarked'])
X_preprocessed = pd.DataFrame(X_encoded.toarray(), columns=encoded_columns)

# Add imputed and scaled features
X_preprocessed['Age'] = X_imputed
X_preprocessed[['Pclass', 'SibSp', 'Parch', 'Fare']] = X_scaled

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)

# Initialize and train multiple models
models = {
    'Random Forest': RandomForestClassifier(),
    'Support Vector Machine': SVC(),
    'Logistic Regression': LogisticRegression()
}

for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions on the test set
    y_pred = model.predict(X_test)
    
    # Evaluate the model performance using relevant metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    # Print the evaluation metrics
    print(f"Model: {name}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-score: {f1:.2f}")
    print()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Load the dataset
data = pd.read_csv('titanic_train.csv')

# Select features and target variable
X = data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex', 'Embarked']]
y = data['Survived']

# Preprocess categorical data (Sex and Embarked)
X_encoded = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)

# Handle missing values in 'Age' and 'Fare' columns
X_encoded['Age'] = X_encoded['Age'].fillna(X_encoded['Age'].mean())
X_encoded['Fare'] = X_encoded['Fare'].fillna(X_encoded['Fare'].mean())

# Scale numerical data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']])

# Combine encoded and scaled features
X_preprocessed = pd.concat([pd.DataFrame(X_scaled, columns=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),
                            X_encoded[['Sex_male', 'Embarked_Q', 'Embarked_S']]], axis=1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)

